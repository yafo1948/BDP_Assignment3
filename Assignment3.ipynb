{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Platform\n",
    "## Assignment 3: ServerLess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [],
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (901559905.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"/var/folders/ps/z0ml3t995m11654nrp_lsc2cv7w2kc/T/ipykernel_62638/901559905.py\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    **By:**\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of this assignment is to:**\n",
    "- Understand and practice the details of Serverless\n",
    "\n",
    "**Instructions:**\n",
    "- Students will form teams of two people each, and submit a single homework for each team.\n",
    "- The same score for the homework will be given to each member of your team.\n",
    "- Your solution is in the form of a Jupyter notebook file (with extension ipynb).\n",
    "- Images/Graphs/Tables should be submitted inside the notebook.\n",
    "- The notebook should be runnable and properly documented. \n",
    "- Please answer all the questions and include all your code.\n",
    "- You are expected to submit a clear and pythonic code.\n",
    "- You can change functions signatures/definitions.\n",
    "\n",
    "**Submission:**\n",
    "- Submission of the homework will be done via Moodle by uploading (not Zip):\n",
    "    - Jupyter Notebook\n",
    "    - 2 Log files\n",
    "    - Additional local scripts\n",
    "- The homework needs to be entirely in English.\n",
    "- The deadline for submission is on Moodle.\n",
    "- Late submission won't be allowed.\n",
    "\n",
    "  \n",
    "- In case of identical code submissions - both groups will get a Zero. \n",
    "- Some groups might be selected randomly to present their code.\n",
    "\n",
    "**Requirements:**  \n",
    "- Python 3.6 should be used.  \n",
    "- You should implement the algorithms by yourself using only basic Python libraries (such as numpy,pandas,etc.)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grading:**\n",
    "- Q0 - 10 points - Setup\n",
    "- Q1 - 40 points - Serverless MapReduceEngine\n",
    "- Q2 - 20 points - MapReduce job to calculate inverted index\n",
    "- Q3 - 30 points - Shuffle\n",
    "\n",
    "`Total: 100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.python.org/simple/\r\n",
      "Requirement already satisfied: ibm-cos-sdk in ./venv/lib/python3.8/site-packages (2.11.0)\r\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.11.0 in ./venv/lib/python3.8/site-packages (from ibm-cos-sdk) (2.11.0)\r\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.11.0 in ./venv/lib/python3.8/site-packages (from ibm-cos-sdk) (2.11.0)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in ./venv/lib/python3.8/site-packages (from ibm-cos-sdk) (0.10.0)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./venv/lib/python3.8/site-packages (from ibm-cos-sdk-core==2.11.0->ibm-cos-sdk) (2.8.2)\r\n",
      "Requirement already satisfied: requests<3.0,>=2.26 in ./venv/lib/python3.8/site-packages (from ibm-cos-sdk-core==2.11.0->ibm-cos-sdk) (2.27.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.26.7 in ./venv/lib/python3.8/site-packages (from ibm-cos-sdk-core==2.11.0->ibm-cos-sdk) (1.26.8)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==2.11.0->ibm-cos-sdk) (1.16.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3.0,>=2.26->ibm-cos-sdk-core==2.11.0->ibm-cos-sdk) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./venv/lib/python3.8/site-packages (from requests<3.0,>=2.26->ibm-cos-sdk-core==2.11.0->ibm-cos-sdk) (2.0.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3.0,>=2.26->ibm-cos-sdk-core==2.11.0->ibm-cos-sdk) (2021.10.8)\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install --quiet zipfile36\n",
    "# !pip install names\n",
    "# !pip install numpy\n",
    "# !pip install scipy\n",
    "# !pip install pandas\n",
    "# !pip install lithops\n",
    "\n",
    "!pip install ibm-cos-sdk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "import ibm_boto3\n",
    "from ibm_botocore.client import Config, ClientError\n",
    "\n",
    "from lithops import FunctionExecutor\n",
    "from lithops import Storage\n",
    "\n",
    "# general\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import threading\n",
    "from threading import Thread\n",
    "import random\n",
    "import warnings\n",
    "import threading # you can use easier threading packages\n",
    "\n",
    "# ml\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# visual\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# notebook\n",
    "from IPython.display import display\n",
    "\n",
    "#random last names\n",
    "import names\n",
    "\n",
    "#SQL\n",
    "import sqlite3\n",
    "from sqlite3 import Error"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "random.seed(123)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 0\n",
    "## Setup\n",
    "\n",
    "1. Navigate to IBM Cloud and open a trial account. No need to provide a credit card\n",
    "2. Choose IBM Cloud Object Storage service from the catalog\n",
    "3. Create a new bucket in IBM Cloud Object Storage\n",
    "4. Create credentials for the bucket with HMAC (access key and secret key)\n",
    "5. Choose IBM Cloud Functions service from the catalog and create a service\n",
    "\n",
    "\n",
    "#### Lithops setup\n",
    "1. By using “git” tool, install master branch of the Lithops project from\n",
    "https://github.com/lithops-cloud/lithops\n",
    "2. Follow Lithops documentation and configure Lithops against IBM Cloud Functions and IBM Cloud Object Storage\n",
    "3. Configure Lithops log level to be in DEBUG mode\n",
    "4. Run Hello World example by using Futures API and verify all is working properly.\n",
    "\n",
    "\n",
    "#### IBM Cloud Object Storage setup\n",
    "1. Upload all the input CSV files that you used in homework 2 into the bucket you created in IBM Cloud Object Storage\n",
    "\n",
    "\n",
    "<br><br><br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def hello(name, number):\n",
    "    return f'hello {name} {number}'\n",
    "\n",
    "\n",
    "def test():\n",
    "    with FunctionExecutor() as fexec:\n",
    "        fut = fexec.call_async(hello, ('World', 1))\n",
    "        print(fut.result())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:51:57,210 [INFO] lithops.config -- Lithops v2.5.8\n",
      "2022-01-08 18:51:57,211 [DEBUG] lithops.config -- Loading configuration from /Users/rludan/git/BigDataHW3/.lithops_config\n",
      "2022-01-08 18:51:57,214 [DEBUG] lithops.config -- Loading Serverless backend module: ibm_cf\n",
      "2022-01-08 18:51:57,260 [DEBUG] lithops.config -- Loading Storage backend module: ibm_cos\n",
      "2022-01-08 18:51:57,333 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Creating IBM COS client\n",
      "2022-01-08 18:51:57,334 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Set IBM COS Endpoint to https://s3.eu-de.cloud-object-storage.appdomain.cloud\n",
      "2022-01-08 18:51:57,335 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Using access_key and secret_key\n",
      "2022-01-08 18:51:57,658 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS client created - Region: eu-de\n",
      "2022-01-08 18:51:57,659 [DEBUG] lithops.serverless.backends.ibm_cf.ibm_cf -- Creating IBM Cloud Functions client\n",
      "2022-01-08 18:51:57,660 [DEBUG] lithops.serverless.backends.ibm_cf.ibm_cf -- Set IBM CF Namespace to Namespace-V4S\n",
      "2022-01-08 18:51:57,661 [DEBUG] lithops.serverless.backends.ibm_cf.ibm_cf -- Set IBM CF Endpoint to https://eu-gb.functions.cloud.ibm.com\n",
      "2022-01-08 18:51:57,663 [DEBUG] lithops.util.ibm_token_manager -- Using IBM IAM API Key - Reusing Token from local cache\n",
      "2022-01-08 18:51:57,668 [DEBUG] lithops.util.ibm_token_manager -- Token expiry time: 2022-01-08 19:02:03.562648+02:00 - Minutes left: 10\n",
      "2022-01-08 18:51:57,669 [INFO] lithops.serverless.backends.ibm_cf.ibm_cf -- IBM CF client created - Region: eu-gb - Namespace: Namespace-V4S\n",
      "2022-01-08 18:51:57,670 [DEBUG] lithops.invokers -- ExecutorID 842866-0 - Invoker initialized. Max workers: 1200\n",
      "2022-01-08 18:51:57,671 [DEBUG] lithops.invokers -- ExecutorID 842866-0 - Serverless invoker created\n",
      "2022-01-08 18:51:57,671 [DEBUG] lithops.executors -- Function executor for ibm_cf created with ID: 842866-0\n",
      "2022-01-08 18:51:57,672 [INFO] lithops.invokers -- ExecutorID 842866-0 | JobID A000 - Selected Runtime: lithopscloud/ibmcf-python-v38 - 256MB\n",
      "2022-01-08 18:51:57,675 [DEBUG] lithops.storage.storage -- Runtime metadata found in local disk cache\n",
      "2022-01-08 18:51:57,677 [DEBUG] lithops.job.job -- ExecutorID 842866-0 | JobID A000 - Serializing function and data\n",
      "2022-01-08 18:51:57,679 [DEBUG] lithops.job.serialize -- Referenced modules: None\n",
      "2022-01-08 18:51:57,680 [DEBUG] lithops.job.serialize -- Modules to transmit: None\n",
      "2022-01-08 18:51:57,681 [DEBUG] lithops.job.job -- ExecutorID 842866-0 | JobID A000 - Uploading function and modules to the storage backend\n",
      "2022-01-08 18:51:58,335 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- PUT Object lithops.jobs/842866-0/e287737b2018574e1cd0c8567e0c1290.func.pickle - Size: 579.0B - OK\n",
      "2022-01-08 18:51:58,335 [DEBUG] lithops.job.job -- ExecutorID 842866-0 | JobID A000 - Data per activation is < 8.0KiB. Passing data through invocation payload\n",
      "2022-01-08 18:51:58,336 [INFO] lithops.invokers -- ExecutorID 842866-0 | JobID A000 - Starting function invocation: hello() - Total: 1 activations\n",
      "2022-01-08 18:51:58,336 [DEBUG] lithops.invokers -- ExecutorID 842866-0 | JobID A000 - Worker processes: 1 - Chunksize: 1\n",
      "2022-01-08 18:51:58,337 [DEBUG] lithops.invokers -- ExecutorID 842866-0 - Async invoker 0 started\n",
      "2022-01-08 18:51:58,338 [DEBUG] lithops.invokers -- ExecutorID 842866-0 - Async invoker 1 started\n",
      "2022-01-08 18:51:58,338 [DEBUG] lithops.invokers -- ExecutorID 842866-0 | JobID A000 - Free workers: 1200 - Going to run 1 activations in 1 workers\n",
      "2022-01-08 18:51:58,341 [INFO] lithops.invokers -- ExecutorID 842866-0 | JobID A000 - View execution logs at /private/var/folders/ps/z0ml3t995m11654nrp_lsc2cv7w2kc/T/lithops/logs/842866-0-A000.log\n",
      "2022-01-08 18:51:58,341 [DEBUG] lithops.monitor -- ExecutorID 842866-0 - Starting Storage job monitor\n",
      "2022-01-08 18:51:58,342 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Creating IBM COS client\n",
      "2022-01-08 18:51:58,344 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Set IBM COS Endpoint to https://s3.eu-de.cloud-object-storage.appdomain.cloud\n",
      "2022-01-08 18:51:58,344 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Using access_key and secret_key\n",
      "2022-01-08 18:51:58,352 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS client created - Region: eu-de\n",
      "2022-01-08 18:51:58,890 [DEBUG] lithops.invokers -- ExecutorID 842866-0 | JobID A000 - Calls 00000 invoked (0.550s) - Activation ID: 3c361822bf744f98b61822bf74ff984a\n",
      "2022-01-08 18:52:00,632 [DEBUG] lithops.monitor -- ExecutorID 842866-0 - Pending: 0 - Running: 1 - Done: 0\n",
      "2022-01-08 18:52:02,049 [DEBUG] lithops.future -- ExecutorID 842866-0 | JobID A000 - Got status from call 00000 - Activation ID: 3c361822bf744f98b61822bf74ff984a - Time: 0.23 seconds\n",
      "2022-01-08 18:52:02,187 [DEBUG] lithops.future -- ExecutorID 842866-0 | JobID A000 - Got output from call 00000 - Activation ID: 3c361822bf744f98b61822bf74ff984a\n",
      "2022-01-08 18:52:02,189 [DEBUG] lithops.invokers -- ExecutorID 842866-0 - Stopping async invokers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello World 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-08 18:52:02,191 [DEBUG] lithops.invokers -- ExecutorID 842866-0 - Async invoker 1 finished\n",
      "2022-01-08 18:52:02,191 [DEBUG] lithops.invokers -- ExecutorID 842866-0 - Async invoker 0 finished\n"
     ]
    }
   ],
   "source": [
    "test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "DB_FILE_NAME='mydb.db'\n",
    "TEMP_FOLDER='./mapreducetemp'\n",
    "FINAL_FOLDER='./mapreducefinal'\n",
    "NUM_OF_RECORDS = 10\n",
    "TEMP_RESULTS_TBL='temp_results'\n",
    "BUCKET_NAME = \"cloud-object-storage-8k-cos-standard-7nq\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv0.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv1.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv2.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv3.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv4.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv5.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv6.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv7.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv8.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv9.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv10.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv11.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv12.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv13.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv14.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv15.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv16.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv17.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv18.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv19.csv\n"
     ]
    }
   ],
   "source": [
    "def upload_csv_to_bucket(path, key_name):\n",
    "    try:\n",
    "        cos.upload_file(Filename=path, Bucket=(\"%s\" % BUCKET_NAME), Key='input/' + str(key_name))\n",
    "        print(f\"Uploaded: {path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Unable to create text file: {0}\".format(e))\n",
    "\n",
    "\n",
    "def seeder(number):\n",
    "    firstname = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "    city = ['NewYork', 'Haifa', 'Munchen', 'London', 'PaloAlto',  'TelAviv', 'Kiev', 'Hamburg']\n",
    "    secondname = []\n",
    "    for i in range(10):\n",
    "        rand_name = names.get_last_name()\n",
    "        secondname.append(rand_name)\n",
    "    df = pd.DataFrame()\n",
    "    df[\"firstname\"] = np.random.choice(firstname, NUM_OF_RECORDS)\n",
    "    df[\"secondname\"] = np.random.choice(secondname, NUM_OF_RECORDS)\n",
    "    df[\"city\"] = np.random.choice(city, NUM_OF_RECORDS)\n",
    "\n",
    "    df.to_csv('./MyCsv%s.csv' % number, index=False)\n",
    "\n",
    "    abspath = os.path.abspath('./MyCsv%s.csv' % number)\n",
    "    key_name = os.path.basename(abspath)\n",
    "    upload_csv_to_bucket(abspath, key_name)\n",
    "\n",
    "\n",
    "def create_cos_client():\n",
    "    global cos\n",
    "    # Constants for IBM COS values\n",
    "    COS_ENDPOINT = \"https://s3.eu-de.cloud-object-storage.appdomain.cloud\"\n",
    "    COS_API_KEY_ID = \"dgKXDSwl3_BYgsotjk6MzYdQhAA0HWVcHSajybwYJblB\"\n",
    "    COS_INSTANCE_CRN = \"crn:v1:bluemix:public:cloud-object-storage:global:a/a306dcbad13145068a4e3898eb5928f5:a155c1bc-bddb-4006-9826-1f72f5fe243b:bucket:cloud-object-storage-8k-cos-standard-7nq\"\n",
    "    # Create client\n",
    "    cos = ibm_boto3.client(\"s3\",\n",
    "                           ibm_api_key_id=COS_API_KEY_ID,\n",
    "                           ibm_service_instance_id=COS_INSTANCE_CRN,\n",
    "                           endpoint_url=COS_ENDPOINT,\n",
    "                           config=Config(signature_version=\"oauth\")\n",
    "                           )\n",
    "\n",
    "create_cos_client()\n",
    "\n",
    "for i in range (20):\n",
    "    seeder(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "## Serverless MapReduceEngine\n",
    "\n",
    "Modify MapReduceEngine from homework 2 into the MapReduceServerlessEngine where map and reduce tasks executed as a serverless actions, instead of local threads. In particular:\n",
    "1. Deploy all map tasks as a serverless actions by using Lithops against IBM Cloud Functions.\n",
    "2. Collect results from all map tasks and store them in the same SQLite as you used in MapReduceEngine and use the same code for the sort and shuffle phase.\n",
    "3. Deploy reduce tasks by using Lithops against IBM Cloud Functions. Instead of persisting results from reduce tasks, return results back to the MapReduceServerlessEngine and proceed with the same workflow as in MapReduceEngine\n",
    "4. Return results of reduce tasks to the user\n",
    "\n",
    "**Please attach:**  \n",
    "Text file with all log messages Lithops printed to console during the execution. Make\n",
    "sure log level is set to DEBUG mode.\n",
    "\n",
    "#### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder(s) already exist(s): [Errno 17] File exists: './mapreducetemp'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(TEMP_FOLDER)\n",
    "    os.mkdir(FINAL_FOLDER)\n",
    "except Exception as e:\n",
    "    print(f\"folder(s) already exist(s): {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_create_temp_results_table = \"\"\"CREATE TABLE IF NOT EXISTS temp_results (\n",
    "                                    key text,\n",
    "                                    value text\n",
    "                                    ); \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_group_by_key = \"\"\"SELECT key, GROUP_CONCAT(value)\n",
    "                      FROM temp_results GROUP BY key ORDER BY (key);\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_drop_all_tables = \"\"\"DROP TABLE temp_results;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_temp_tables(conn):\n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(sql_drop_all_tables)\n",
    "    except Error as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to a SQLite database \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        print(sqlite3.version)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(conn, create_table_sql):\n",
    "    \"\"\" create a table from the create_table_sql statement\n",
    "    :param conn: Connection object\n",
    "    :param create_table_sql: a CREATE TABLE statement\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(create_table_sql)\n",
    "    except Error as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grouped_values(conn):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql_group_by_key)\n",
    "\n",
    "    rows = cur.fetchall()\n",
    "        \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "connection = create_connection(DB_FILE_NAME)\n",
    "create_table(connection, sql_create_temp_results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "if connection is not None:\n",
    "    # create temp_results table\n",
    "    create_table(connection, sql_create_temp_results_table)\n",
    "else:\n",
    "    print(\"Error! cannot create the database connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreadExecution(Thread):\n",
    "    def __init__(self, group=None, target=None, name=None,\n",
    "                 args=(), kwargs={}, Verbose=None):\n",
    "        Thread.__init__(self, group, target, name, args, kwargs)\n",
    "        self._return = None\n",
    "        \n",
    "    def run(self):\n",
    "        if self._target is not None:\n",
    "            self._return = self._target(*self._args,\n",
    "                                                **self._kwargs)\n",
    "    def join(self, *args):\n",
    "        Thread.join(self, *args)\n",
    "        return self._return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapReduceServerlessEngine():\n",
    "    def execute(self, input_data, map_function, reduce_function, params):\n",
    "        curr_map = 0\n",
    "\n",
    "        for input_key in input_data:\n",
    "            with FunctionExecutor() as fexec:\n",
    "                fut = fexec.call_async(func=map_function, data=(input_key, params['column'])) # {\"key\":key, \"col\":0}\n",
    "                map_result = fut.result()\n",
    "                print(map_result)\n",
    "\n",
    "                if map_result is not None:\n",
    "                    map_result_df = pd.DataFrame(map_result, columns=[\"key\", \"value\"])\n",
    "                    map_result_df.to_csv(TEMP_FOLDER + '/part-tmp-%s.csv' % str(curr_map), index=False, header=True)\n",
    "            curr_map += 1\n",
    "\n",
    "        for temp_file_name in os.scandir(TEMP_FOLDER):\n",
    "            csv_df = pd.read_csv(temp_file_name.path)\n",
    "            csv_df.to_sql(TEMP_RESULTS_TBL, connection, if_exists='append', index=False)\n",
    "        \n",
    "        grouped_values = get_grouped_values(connection)\n",
    "\n",
    "        curr_reduce = 0\n",
    "        for reduce_value in grouped_values:\n",
    "            with FunctionExecutor() as fexec:\n",
    "                fut = fexec.call_async(reduce_function, (reduce_value[0], reduce_value[1]))\n",
    "                reduce_result = fut.result()\n",
    "                print(reduce_result)\n",
    "\n",
    "                if reduce_result is not None:\n",
    "                    result_df = pd.DataFrame(reduce_result, columns=[\"values\"])\n",
    "                    result_df.to_csv(FINAL_FOLDER + '/part-%s-final.csv' % curr_reduce, index=False, header=True)\n",
    "            curr_reduce += 1\n",
    "            \n",
    "        return \"MapReduce Completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_map(key, col):\n",
    "    storage = Storage()\n",
    "    buffer = storage.get_object(BUCKET_NAME, key, stream=True)\n",
    "\n",
    "    values = pd.read_csv(filepath_or_buffer = buffer, usecols=[col], skiprows=1)\n",
    "\n",
    "    return [(x[0], key) for x in values.to_records(index=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_reduce(value, documents):\n",
    "    ret_val = [value]\n",
    "    temp_set = set(documents.split(','))\n",
    "    ret_val.extend(temp_set)\n",
    "\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 01:04:37,851 [DEBUG] lithops.config -- Loading configuration from /Users/rludan/git/BigDataHW3/.lithops_config\n",
      "2022-01-10 01:04:37,860 [DEBUG] lithops.config -- Loading Storage backend module: ibm_cos\n",
      "2022-01-10 01:04:37,861 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Creating IBM COS client\n",
      "2022-01-10 01:04:37,861 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Set IBM COS Endpoint to https://s3.eu-de.cloud-object-storage.appdomain.cloud\n",
      "2022-01-10 01:04:37,862 [DEBUG] lithops.storage.backends.ibm_cos.ibm_cos -- Using access_key and secret_key\n",
      "2022-01-10 01:04:37,872 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS client created - Region: eu-de\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input/MyCsv0.csv', 'input/MyCsv1.csv', 'input/MyCsv10.csv', 'input/MyCsv11.csv', 'input/MyCsv12.csv', 'input/MyCsv13.csv', 'input/MyCsv14.csv', 'input/MyCsv15.csv', 'input/MyCsv16.csv', 'input/MyCsv17.csv', 'input/MyCsv18.csv', 'input/MyCsv19.csv', 'input/MyCsv2.csv', 'input/MyCsv3.csv', 'input/MyCsv4.csv', 'input/MyCsv5.csv', 'input/MyCsv6.csv', 'input/MyCsv7.csv', 'input/MyCsv8.csv', 'input/MyCsv9.csv']\n"
     ]
    }
   ],
   "source": [
    "storage = Storage()\n",
    "input_data =  storage.list_keys(BUCKET_NAME, prefix='input/')\n",
    "\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "mapreduce = MapReduceServerlessEngine()\n",
    "status = mapreduce.execute(input_data, inverted_map, inverted_reduce, params={'column':0})\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for file_name in os.scandir(TEMP_FOLDER):\n",
    "    os.remove(file_name.path)\n",
    "drop_temp_tables(connection)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "## Submit MapReduce job to calculate inverted index\n",
    "1. Use input_data: `cos://bucket/<path to CSV data>`\n",
    "2. Submit MapReduce job with reduce and map functions as you used in homework 2, as follows\n",
    "\n",
    "    `mapreduce = MapReduceServerlessEngine()`  \n",
    "    `results = mapreduce.execute(input_data, inverted_map, inverted_index)`   \n",
    "    `print(results)`\n",
    "\n",
    "**Please attach:**  \n",
    "Text file with all log messages Lithops printed to console during the execution. Make\n",
    "sure log level is set to DEBUG mode.\n",
    "\n",
    "#### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "## Shuffle\n",
    "\n",
    "MapReduceServerlessEngine deploys both map and reduce tasks as serverless invocations.   \n",
    "However, once map stage completed, the result are transferred from the map tasks to the SQLite database located on the client machine (laptop in your case), then performed local shuffle and then invoked reduce tasks passing them relevant parameters.\n",
    "\n",
    "(To support your answers, feel free to use examples, Images, etc.)\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Explain why this approach is not efficient and what are cons and pros of such architecture in general. In broader scope you may assume that MapReduceServerlessEngine executed in some powerful machine and not just laptop.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\<your answer here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "**2. Suggest how can you improve shuffle so intermediate data will not be downloaded to the client at all and shuffle performed in the cloud as well. Explain pros and cons of the approaches you suggest.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\<your answer here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "**3. Can you make serverless shuffle?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\<your answer here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "Good Luck :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}